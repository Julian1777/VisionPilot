
# ğŸš— Self-Driving Car Simulation: Computer Vision, Deep Learning & Real-Time Perception (BeamNG.tech)

<p align="center">
  <a href="https://star-history.com/#Julian1777/self-driving-project&Date">
    <img src="https://api.star-history.com/svg?repos=Julian1777/self-driving-project&type=Date" alt="Star History Chart" />
  </a>
</p>

A modular Python project for autonomous driving research and prototyping, fully integrated with the BeamNG.tech simulator and Foxglove visualization. This system combines traditional computer vision and state-of-the-art deep learning (CNN, U-Net, YOLO, SCNN) with real-time sensor fusion and autonomous vehicle control to tackle:

- ğŸ›£ï¸ Lane detection (Traditional CV, SCNN, capable of city & highway scenarios)
- ğŸ›‘ Traffic sign classification & detection (CNN, YOLOv8)
- ğŸš¦ Traffic light detection & classification (YOLOv8, CV, CNN)
- ğŸš— Vehicle & pedestrian detection and recognition (YOLOv8)
- ğŸ“¡ Multi-sensor fusion (Camera, LiDAR, Radar)
- ğŸ§  Multi-model inference, real-time simulation, autonomous driving with PID control (BeamNG.tech)
- ğŸ“Š Real-time visualization and monitoring (Foxglove WebSocket)

Features robust training pipelines, modular sensor integration, multi-model inference, and a flexible folder structure for easy experimentation and extension. The project is designed for research and prototyping in realistic driving environments using BeamNG.tech with professional-grade visualization through Foxglove.



## ğŸ¥ Demos

Below are sample demos of the system's capabilities. More demos (including new models and tasks) will be added as development progresses.

| Lane Detection (CV) | Lane Detection (Neural Net) |
|---------------------|----------------------------|
| ![lane-cv](assets/lane_cv.gif) <br> *(coming soon)* | ![lane-nn](assets/lane_nn.gif) <br> *(coming soon)* |

| Sign Detection/Classification | Traffic Light Detection/Classification |
|------------------------------|---------------------------------------|
| ![sign](assets/sign.gif) <br> *(detection & classification)* | ![light](assets/light.gif) <br> *(detection & classification)* |

| Vehicle/Object/Pedestrian Detection | |
|-------------------------------------|--|
| ![vehicle](assets/vehicle.gif) <br> *(coming soon)* | |

> More demo videos and visualizations will be added as features are completed and models are improved.



## ğŸ”§ Features

- Lane detection with SCNN and traditional OpenCV
- Traffic Sign Classification + Detection
- Traffic Light Classification + Detection
- Vehicle & Pedestrian Detection
- Multi-sensor fusion (Camera, LiDAR, Radar)
- Real-time autonomous driving with PID control
- Cruise control
- Real-time visualization via Foxglove WebSocket
- Modular configuration system (YAML-based)
- Drive logging and telemetry
- Support for multiple scenarios (highway, city)


## ğŸ› ï¸ Built With

- **Simulation:** BeamNG.tech (https://www.beamng.tech/)
- **Visualization:** Foxglove Studio (WebSocket real-time visualization)
- **Deep Learning:** TensorFlow / Keras, PyTorch
- **Computer Vision:** OpenCV, YOLOv8 (Ultralytics)
- **Language:** Python 3.8+
- **Control Systems:** PID controllers, sensor fusion


## ğŸ“š Datasets Used

- **CU Lane Dataset** for lane segmentation
- **DLDT / LISA** for traffic light classification & detection
- **Mapillary** for sign detection
- **BDD** for vehicle and pedestrian detection

## ğŸ“Š Results

For qualitative and quantitative results, see the demo section above and the `results/` folder for visualizations, metrics, and sample outputs. Example outputs include:

  - `results/traffic-sign-classification/metrics/` (JSON, curves)
  - `results/traffic-sign-detection/weights/` (YOLO checkpoints)
  - `results/vehicle-pedestrian/visualizations/` (confusion matrices, sample batches)


## âš¡ Quickstart & Usage

1. **Install dependencies:**
  ```bash
  pip install -r requirements.txt
  ```

2. **Configure simulation (Optional):**
  Edit configuration files in `beamng_sim/config/`:
  - `beamng_sim.yaml` - BeamNG host, port, and vehicle settings
  - `scenarios.yaml` - Available scenarios
  - `sensors.yaml` - Sensor parameters (camera, LiDAR, radar)
  - `control.yaml` - PID tuning and vehicle control parameters
  
  See `beamng_sim/config/README.md` for detailed parameter descriptions.

3. **Run the simulation:**
  ```bash
  python -m beamng_sim.beamng
  ```
  - Make sure BeamNG.tech is installed, running, and properly licensed. See [BeamNG.tech documentation](https://www.beamng.tech/) for setup.
  - Foxglove visualization will be available at `ws://localhost:8765`

4. **View real-time visualization:**
  - Open [Foxglove Studio](https://app.foxglove.dev/)
  - Connect to WebSocket server: `ws://localhost:8765`
  - Load the provided Foxglove layout or create your own

  > **Important:** You must ensure that all required models (e.g., trained weights, .h5/.pt files) and configuration files are placed in the correct directories as expected by the code. The folder structure shown below must be followed, and missing files or incorrect paths will cause errors. See each module's README or script comments for details on required files and their locations.

5. **Train a model:**
  See notebooks or scripts in each module folder.

  > **Note:** You must download and prepare the required datasets yourself (e.g., sorting, cropping, formatting, or converting to the expected structure) as described in each module's documentation or script. The code will not work without properly prepared data.


## ğŸ“ Setup & Installation
- Python 3.8+
- See `requirements.txt` for all dependencies
- Required: BeamNG.tech simulator for real-time testing ([Download & License](https://www.beamng.tech/))


## ğŸ§  Model Details
All models are located in the models folder
- **Lane Detection:** SCNN
- **Traffic Sign Detect/Class:** CNN classifier, YOLOv8 detector
- **Traffic Light Detect/Class:** YOLOv8 detector, CNN classifier
- **Vehicle/Pedestrian:** YOLOv8

## ğŸ“‚ Folder Structure

> **Currently Outdated**
<details>
  <summary>Click to expand folder structure</summary>


```
self-driving-project/
â”œâ”€â”€ beamng_sim/                          # BeamNG.tech simulation & real-time perception
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ beamng.py                        # Main BeamNG.tech interface/entry point
â”‚   â”œâ”€â”€ drive_log/                       # Simulation drive logs (CSV)
â”‚   â”œâ”€â”€ debug_output/
â”‚   â”‚   â””â”€â”€ alotofnoise/                 # Debug images for lane detection, perspective
â”‚   â”‚
â”‚   â”œâ”€â”€ lane_detection/                  # Lane detection algorithms
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                      # Process frames (CV, UNet, SCNN)
â”‚   â”‚   â”œâ”€â”€ fusion.py                    # Multi-model fusion logic
â”‚   â”‚   â”œâ”€â”€ perspective.py               # Bird's eye view transformation
â”‚   â”‚   â”œâ”€â”€ metrics.py                   # Lane metrics calculation
â”‚   â”‚   â”œâ”€â”€ color_threshold_debug.py
â”‚   â”‚   â”œâ”€â”€ lane_finder.py
â”‚   â”‚   â”œâ”€â”€ thresholding.py
â”‚   â”‚   â”œâ”€â”€ visualization.py
â”‚   â”‚   â”œâ”€â”€ old_lane_detection.py
â”‚   â”‚   â””â”€â”€ scnn/                        # SCNN model files
â”‚   â”‚       â””â”€â”€ scnn_model.py
â”‚   â”‚
â”‚   â”œâ”€â”€ lidar/                           # LiDAR sensor processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                      # LiDAR frame processing
â”‚   â”‚   â”œâ”€â”€ lidar.py
â”‚   â”‚   â”œâ”€â”€ lidar_testing.py
â”‚   â”‚   â””â”€â”€ visualization_tool.py
â”‚   â”‚
â”‚   â”œâ”€â”€ radar/                           # Radar sensor processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                      # Radar frame processing
â”‚   â”‚   â””â”€â”€ radar.py
â”‚   â”‚
â”‚   â”œâ”€â”€ sign/                            # Traffic sign detection & classification
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ detect_classify.py
â”‚   â”‚   â””â”€â”€ augmentation.py              # Data augmentation (random_brightness, etc)
â”‚   â”‚
â”‚   â”œâ”€â”€ vehicle_obstacle/                # Vehicle & pedestrian detection
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ vehicle_obstacle_detection.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                           # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ pid_controller.py            # PID control for steering/speed
â”‚   â”‚
â”‚   â””â”€â”€ traffic_lights/                  # Traffic light detection & classification
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py
â”‚       â””â”€â”€ detection.py
â”‚
â”œâ”€â”€ config/                              # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                        # Global config (paths, models, calibration)
â”‚   â”œâ”€â”€ README.md                        # ğŸ“– Configuration guide - descriptions of each YAML file
â”‚   â”œâ”€â”€ beamng_sim.yaml                  # Simulation, vehicles, and scenarios config
â”‚   â”œâ”€â”€ scenarios.yaml                   # Scenario definitions (highway, city, etc)
â”‚   â”œâ”€â”€ sensors.yaml                     # Sensor configs (camera, lidar, radar)
â”‚   â””â”€â”€ control.yaml                     # Vehicle control & PID tuning parameters
â”‚
â”œâ”€â”€ lane-detection/                      # Traditional CV lane detection (standalone)
â”‚   â”œâ”€â”€ city/                            # City-specific algorithms
â”‚   â””â”€â”€ highway/                         # Highway-specific algorithms
â”‚
â”œâ”€â”€ lane-detection-cnn/                  # CNN/SCNN lane detection training
â”‚   â”œâ”€â”€ lane_detection.py                # Model training & evaluation
â”‚   â”œâ”€â”€ dataset/                         # CULane dataset
â”‚   â””â”€â”€ results/                         # Training outputs
â”‚
â”œâ”€â”€ traffic_sign/                        # Traffic sign detection & classification
â”‚   â”œâ”€â”€ detection_kaggle.py              # YOLO training script
â”‚   â”œâ”€â”€ realtime.py                      # Real-time inference
â”‚   â”œâ”€â”€ dataset/                         # Mapillary dataset
â”‚   â””â”€â”€ results/
â”‚
â”œâ”€â”€ traffic-lights/                      # Traffic light detection & classification
â”‚   â”œâ”€â”€ detection.py                     # YOLO training script
â”‚   â”œâ”€â”€ classification.py                # CNN classification training
â”‚   â”œâ”€â”€ dataset_verification.py          # Dataset validation
â”‚   â”œâ”€â”€ yolo_test.py                     # Testing & evaluation
â”‚   â”œâ”€â”€ dtld_dataset/                    # DTLD dataset (German Traffic Lights)
â”‚   â”œâ”€â”€ lisa_dataset/                    # LISA dataset (US Traffic Lights)
â”‚   â”œâ”€â”€ yolo_dataset/                    # Prepared YOLO format dataset
â”‚   â””â”€â”€ results/                         # Training outputs
â”‚
â”œâ”€â”€ vehicle-pedestrian-detection/        # Vehicle & pedestrian detection
â”‚   â”œâ”€â”€ training.py
â”‚   â”œâ”€â”€ dataset/                         # BDD100K dataset
â”‚   â””â”€â”€ results/
â”‚
â”œâ”€â”€ models/                              # Pretrained models
â”‚   â”œâ”€â”€ lane_detection_unet.h5           # U-Net lane detection
â”‚   â”œâ”€â”€ scnn.pth                         # SCNN lane detection
â”‚   â”œâ”€â”€ sign_detection.pt                # YOLOv8 sign detection
â”‚   â”œâ”€â”€ sign_classification.h5           # CNN sign classifier
â”‚   â”œâ”€â”€ vehicle_pedestrian_detection.pt  # YOLOv8 vehicle/pedestrian
â”‚   â”œâ”€â”€ traffic_light_detect_class.pt    # YOLOv8 traffic light
â”‚   â””â”€â”€ camera_calibration.pkl           # Camera calibration data
â”‚
â”œâ”€â”€ datasets/                            # All datasets (organized)
â”‚   â”œâ”€â”€ lane-detection/
â”‚   â”‚   â”œâ”€â”€ culane/                      # CULane dataset
â”‚   â”‚   â”œâ”€â”€ cityscapes/
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”œâ”€â”€ traffic-light/
â”‚   â”‚   â”œâ”€â”€ dtld/
â”‚   â”‚   â”œâ”€â”€ lisa/
â”‚   â”‚   â””â”€â”€ merged/
â”‚   â”œâ”€â”€ traffic-sign/
â”‚   â”‚   â”œâ”€â”€ mapillary/
â”‚   â”‚   â””â”€â”€ gtsrb/
â”‚   â””â”€â”€ vehicle-pedestrian/
â”‚       â”œâ”€â”€ bdd100k/
â”‚       â””â”€â”€ processed/
â”‚
â”œâ”€â”€ results/                             # Training & experiment results
â”‚   â”œâ”€â”€ lane-detection/
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ visualizations/
â”‚   â”œâ”€â”€ traffic-sign/
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ visualizations/
â”‚   â”œâ”€â”€ traffic-light/
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ visualizations/
â”‚   â””â”€â”€ vehicle-pedestrian/
â”‚       â”œâ”€â”€ metrics/
â”‚       â””â”€â”€ visualizations/
â”‚
â”œâ”€â”€ images/                              # Sample images & predictions
â”‚   â”œâ”€â”€ lane-detection/
â”‚   â”œâ”€â”€ traffic-signs/
â”‚   â”œâ”€â”€ traffic-lights/
â”‚   â””â”€â”€ vehicle-pedestrian/
â”‚
â”œâ”€â”€ videos/                              # Video clips for testing/demo
â”‚   â”œâ”€â”€ lane-detection/
â”‚   â”œâ”€â”€ traffic-lights/
â”‚   â””â”€â”€ simulation/
â”‚
â”œâ”€â”€ notebooks/                           # Jupyter notebooks (experiments, analysis)
â”‚   â”œâ”€â”€ collab/
â”‚   â”‚   â””â”€â”€ traffic_sign_detection.ipynb
â”‚   â”œâ”€â”€ kaggle/
â”‚   â”‚   â””â”€â”€ traffic-sign-detection.ipynb
â”‚   â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ training/
â”‚   â””â”€â”€ visualization/
â”‚
â”œâ”€â”€ assets/                              # Project assets (GIFs, diagrams, etc)
â”‚   â”œâ”€â”€ lane_cv.gif
â”‚   â”œâ”€â”€ lane_nn.gif
â”‚   â”œâ”€â”€ sign.gif
â”‚   â”œâ”€â”€ light.gif
â”‚   â”œâ”€â”€ vehicle.gif
â”‚   â””â”€â”€ architecture_diagram.png
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”œâ”€â”€ README.md                            # Project documentation (this file)
â”œâ”€â”€ LICENSE
â””â”€â”€ setup.py                             # Package setup (optional)
```

</details>

> Descriptions of the configuration files can be found in the `config/README.md` file.

## ğŸš€ Roadmap

- [x] Sign classification & Detection (CNN)
- [x] Traffic light classification & Detection
- [x] Lane detection (SCNN, CV)
- [x] â­ Advanced lane detection using OpenCV (robust city/highway, lighting, outlier handling)
- [x] Integrate and test in BeamNG.tech simulation (replacing CARLA)
- [x] Tweak lane detection parameters and thresholds
- [x] â­ Integrate Radar
- [x] Integrate Lidar
- [ ] Lidar Object Detection
- [ ] Lidar lane boundry detection
- [x] Modularize and clean up BeamNG.tech pipeline
- [x] â­ Integrate vehicle control (autonomous driving logic)
- [ ] Traffic scenarios: driving in heavy, moderate, and light traffic
- [ ] Test different weather and lighting conditions
- [x] â­ Begin integration of other models (sign, light, pedestrian, etc.)
- [x] â­ Adaptive Cruise Control
- [ ] Emergency Breaking / Collision Avoidance
- [ ] Weather condition detection
- [x] â­ Full Foxglove visualization integration
- [x] â­ Modular YAML configuration system
- [x] â­ Real-time drive logging and telemetry
- [ ] Blindspot Monitoring

**Future / Stretch Goals**
- [ ] Docker containarization
- [ ] SLAM (simultaneous localization and mapping)
- [ ] GPS/IMU sensor
- [ ] Map Matching algorithm
- [ ] ğŸ’¤ Global and Local path planning
- [ ] ğŸ’¤ Behaviour planning and anticipation
- [ ] Test using actual RC car
- [ ] ğŸ’¤ End-to-end driving policy learning (RL, imitation learning)
- [ ] Multi Camera
- [ ] ğŸ’¤ Advanced traffic participant prediction (trajectory, intent)

> â­ = Complete but still being improved/tuned/changed (not final version)

> ğŸ’¤ = Minimal Priority, can be addressed later

## ğŸ™ Credits
- Datasets: CU Lane, LISA, GTRSB, Mapillary, BDD100K
- Models: Ultralytics YOLOv8, custom CNNs
- Simulation: BeamNG.tech ([BeamNG GmbH](https://www.beamng.tech/))
- Special thanks to [Kaggle](https://www.kaggle.com/) for providing free GPU resources for model training without them it would've been imposible to train such good models.

### BeamNG.tech Citation

> **Title:** BeamNG.tech  
> **Author:** BeamNG GmbH  
> **Address:** Bremen, Germany  
> **Year:** 2025  
> **Version:** 0.35.0.0  
> **URL:** https://www.beamng.tech/
